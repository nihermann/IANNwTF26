{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von hw06.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XOWQj0zGG8TG",
        "j9eERBJLHsWh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihermann/IANNwTF26/blob/master/Kopie_von_hw06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-E-atCXG_xe"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs2FDF87F3Zv"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Dropout, GlobalAveragePooling2D, Dense, AveragePooling2D, Activation,Concatenate\n",
        "from tensorflow.keras.activations import relu, softmax\n",
        "from tensorflow.keras.regularizers import L2"
      ],
      "execution_count": 937,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7BhN6E6F3uN"
      },
      "source": [
        "training_raw, (test_img, test_labels) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 938,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z7Zl8gMF3xA"
      },
      "source": [
        "# split test data again into test and validation data\n",
        "test_raw = (test_img[5000:], test_labels[5000:])\n",
        "validation_raw = (test_img[:5000], test_labels[:5000])"
      ],
      "execution_count": 939,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GmS6DfBHIN-"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFNsKxWbJmkN"
      },
      "source": [
        "# these parameters are used to create a list of indices to later select \n",
        "# the right manipulated dataset for training in each epoch\n",
        "NO_PAIRING = 0\n",
        "WITH_PAIRING = 1\n",
        "SEQUENTIAL_PARING = lambda pairing, no_pairing: [WITH_PAIRING]*pairing + [NO_PAIRING]*no_pairing"
      ],
      "execution_count": 940,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_NA-qfQF3z_"
      },
      "source": [
        "## HYPERPARAMETERS\n",
        "# Data\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER = 50000 # the large shuffle buffer will be very important later!\n",
        "CACHE_PATH = \"/content/drive/MyDrive/\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/\"\n",
        "SAVING_THRESHOLD = 0.4 # minimum accuracy before saving the model\n",
        "\n",
        "# Training\n",
        "# initialize the Training phases sequentially\n",
        "PHASECONTROLLER = [NO_PAIRING]*2 + [WITH_PAIRING]*8 + SEQUENTIAL_PARING(pairing=8, no_pairing=2)*10 + [NO_PAIRING]*5\n",
        "EPOCHS = len(PHASECONTROLLER)\n",
        "LEARNING_RATE = 0.00005\n",
        "\n",
        "# loss\n",
        "LOSS_FUNCTION = tf.keras.losses.categorical_crossentropy\n",
        "# optimizer\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "execution_count": 941,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOWQj0zGG8TG"
      },
      "source": [
        "# Datapipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJDlXFqqF32h"
      },
      "source": [
        "if not os.path.exists(CACHE_PATH[:-1]):\n",
        "    print(\"lol\")\n",
        "    os.makedirs(CACHE_PATH[:-1])"
      ],
      "execution_count": 942,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Y1jugTNod6",
        "outputId": "10aa6b9c-86ee-4164-e2dc-7ee81d61541e"
      },
      "source": [
        "os.listdir(CACHE_PATH[:-1])"
      ],
      "execution_count": 943,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks',\n",
              " 'NI_Lecture_Template (2).gslides',\n",
              " 'TfNN',\n",
              " 'Models',\n",
              " '.data-00000-of-00001',\n",
              " '.index']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 943
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB74UJWPJ0DI"
      },
      "source": [
        "def blend(img1, img2, blending_factor=0.5, prob=0.8):\n",
        "    '''\n",
        "    IN:\n",
        "        img1: image with shape (h,w,c)\n",
        "        img2: imgae with shape (h,w,c)\n",
        "        blending_factor: float in range [0:1], intensity of img1 in the new img\n",
        "        prob: float in range[0:1], probability of success\n",
        "    OUT:\n",
        "        img: result of blending img1 and img2 by blending_factor\n",
        "    '''\n",
        "    if tf.random.uniform([1]) > prob or blending_factor > 1 or blending_factor < 0:\n",
        "        return img1\n",
        "    \n",
        "    return (img1*blending_factor) + img2*(1-blending_factor)"
      ],
      "execution_count": 944,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRQH-t9NJ1f2"
      },
      "source": [
        "def data_augmentation(dataset, horizontal_flip=True, sample_pairing=True):\n",
        "    '''\n",
        "    Possible Augmentations:\n",
        "        horizontal_flip: randomly flips imgs of the dataset horizontally.\n",
        "        sample_pairing: blends two random img together. Twist: there is a 25% chance to skip the whole augmentation.\n",
        "    '''\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "    if horizontal_flip:\n",
        "        dataset = dataset.map(lambda img, label: (tf.image.random_flip_left_right(img), label),\n",
        "                              num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if sample_pairing and tf.random.uniform([1]) < 0.75:\n",
        "        # first split the dataset into its img and label\n",
        "        ds_images = dataset.map(lambda img, label: img)\n",
        "        ds_labels = dataset.map(lambda img, label: label)\n",
        "\n",
        "        # shuffle the img dataset and store it seperately. For better results the buffersize should be big\n",
        "        # enough such that also pictures of different classes will be paired later.\n",
        "        shuffled_ds = ds_images.shuffle(SHUFFLE_BUFFER)\n",
        "\n",
        "        # zip all back together\n",
        "        merged_ds = shuffled_ds.zip((ds_images, shuffled_ds, ds_labels))\n",
        "\n",
        "        # blend two random imgs (pixlewise) with equal(50%:50%) strenght together and use the label of the first one\n",
        "        dataset = merged_ds.map(lambda img1, img2, label1: (blend(img1, img2, blending_factor=0.5, prob=0), label1),\n",
        "                                num_parallel_calls=AUTOTUNE)\n",
        "        \n",
        "    return dataset"
      ],
      "execution_count": 945,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFJ8cXBCF35e"
      },
      "source": [
        "def build_pipeline(data, batchsize, shuffle_buffer, horizontal_flip=False, sample_pairing=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(data)\n",
        "    ds = ds.map(lambda img, label: (2*(img/255)-1, # normalize the color values between [-1:1]\n",
        "                                    tf.reshape(tf.one_hot(label, 10), shape=(-1,)))) # one hot the labels\n",
        "    \n",
        "    # cache the preprocessing for better performance\n",
        "    ds = ds.cache(CACHE_PATH)\n",
        "\n",
        "    # if one of the data augmentations is choosen, apply them.\n",
        "    if horizontal_flip or sample_pairing:\n",
        "        ds = data_augmentation(ds, horizontal_flip, sample_pairing)\n",
        "\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer)\n",
        "    ds = ds.batch(batchsize)\n",
        "\n",
        "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds"
      ],
      "execution_count": 946,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5wbWTqmHMYl"
      },
      "source": [
        "# ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJr0eDf8F3_h"
      },
      "source": [
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "  '''Class for implementing a Residual Block to create a ResNet'''\n",
        "  \n",
        "  def __init__(self,channels, num_layer_iterations = 3,input_dimension = 32):\n",
        "    super(ResidualBlock,self).__init__()\n",
        "\n",
        "    self.bottleneck = Conv2D(filters= channels, kernel_size=(1,1), activation= relu)\n",
        "    conv_layer = Conv2D(filters = channels, kernel_size = 3, padding = \"same\", kernel_regularizer= L2()) #save the default for the convolutional layer in a variable for to keep the list comp. short\n",
        "    self.block_layers = [layer for layer in range(num_layer_iterations-1) for layer in (conv_layer, BatchNormalization()) ] #list comprehension -> building a list by adding a convolutional layers and batchnormalization (alternating) for the wanted number of blocks    \n",
        "    self.block_layers.append(Conv2D(filters = input_dimension, kernel_size = 1, padding = \"same\", kernel_regularizer= L2()))\n",
        "    self.block_layers.append(BatchNormalization())\n",
        "    self.block_layers.append(Activation('relu'))\n",
        "   \n",
        "  @tf.function\n",
        "  def call(self,x,training=True):\n",
        "    x = self.bottleneck(x)#bottleneck layer with kernel_size of 1,1\n",
        "    y = x #save the input\n",
        "    for layer in self.block_layers: #iterate trough all layers in the block_layers list\n",
        "      x = layer(x,training = training)\n",
        "    return x + y #add the original input to the transformed input"
      ],
      "execution_count": 947,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Seww6Dih0s"
      },
      "source": [
        "class ResNet(Model):\n",
        "  '''Class for implementing a Residual Network'''\n",
        "  def __init__(self, block_num = 6, block_channels  = 32, num_layer_iterations = 3):\n",
        "    super(ResNet,self).__init__()        \n",
        " \n",
        "    self.conv_1 = Conv2D(filters= 32, kernel_size= 3, padding= 'same',  kernel_regularizer= L2())\n",
        "    self.batch_1 = BatchNormalization()\n",
        "    self.activation_1 = Activation('relu')\n",
        "    \n",
        "    self.residual_blocks = [block for block in range(block_num) for block in (ResidualBlock(channels = block_channels, num_layer_iterations = num_layer_iterations),MaxPool2D(padding='same'))]\n",
        "    \n",
        "    self.conv_2 = Conv2D(filters= 32, kernel_size= 3, padding= 'same', activation= relu, kernel_regularizer= L2())\n",
        "    self.average = GlobalAveragePooling2D()\n",
        "    self.dense = Dense(units= 10, activation= softmax)\n",
        "\n",
        "  @tf.function\n",
        "  def call(self, input, training = True):\n",
        "    x = self.conv_1(input)\n",
        "    x = self.batch_1(x,training = training)\n",
        "    x = self.activation_1(x)\n",
        "    for layer in self.residual_blocks:\n",
        "      x = layer(x,training = training)\n",
        "    x = self.conv_2(x)\n",
        "    x = self.average(x)\n",
        "    x = self.dense(x)\n",
        "    return x"
      ],
      "execution_count": 948,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38nVPTcKF5L"
      },
      "source": [
        "# DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGx-XbrnKFI5"
      },
      "source": [
        "class TransitionLayer(layers.Layer):\r\n",
        "  '''class for implementing a Transition layer'''\r\n",
        "\r\n",
        "  def __init__(self,num_filters = 32):\r\n",
        "    super(TransitionLayer,self).__init__()\r\n",
        "    self.layer_list = [\r\n",
        "\r\n",
        "      Conv2D(filters = num_filters, kernel_size = 1 , padding = 'same', kernel_regularizer= L2()),             \r\n",
        "      BatchNormalization(),\r\n",
        "      Activation('relu'),\r\n",
        "      AveragePooling2D(strides=(2,2))\r\n",
        "      \r\n",
        "    ]\r\n",
        "  \r\n",
        "  @tf.function\r\n",
        "  def call(self, input, training = True):\r\n",
        "    for layer in self.layer_list:\r\n",
        "      input = layer(input)\r\n",
        "    return input"
      ],
      "execution_count": 949,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k31QBlUzHxeI"
      },
      "source": [
        "class DenseBlock(layers.Layer):\r\n",
        "  ''''Class for implementing a DenseBlock'''\r\n",
        "\r\n",
        "  def __init__(self,num_blocks = 3,block_length = 10):\r\n",
        "    super(DenseBlock,self).__init__()\r\n",
        "    \r\n",
        "    self.layer_list = []\r\n",
        "    for i in range(num_blocks):\r\n",
        "      self.layer_list.append(Conv2D(filters= block_length, kernel_size=(3,3), padding= 'same', kernel_regularizer= L2()))\r\n",
        "      self.layer_list.append(BatchNormalization())\r\n",
        "    \r\n",
        "    self.concat = tf.keras.layers.Concatenate()\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def call(self, input, training= False):\r\n",
        "    for layer in self.layer_list:\r\n",
        "      output = layer(input,training = training)\r\n",
        "      input = self.concat([input, output])\r\n",
        "    return input\r\n"
      ],
      "execution_count": 950,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v7-NeX2Uhz8"
      },
      "source": [
        "class DenseNet(tf.keras.Model):\n",
        "  '''Class dor implementing a DenseNet'''\n",
        "\n",
        "  def __init__(self,num_blocks = 5, block_length =3,growth_rate = 10):\n",
        "    super(DenseNet,self).__init__()\n",
        "\n",
        "    self.conv_1 = Conv2D(filters= (growth_rate * 2),input_shape=(BATCH_SIZE, 32, 32, 3), kernel_size= 3, padding= 'same', kernel_regularizer= L2())\n",
        "    self.batch_1 = BatchNormalization()\n",
        "    self.activation_1 = Activation('relu')\n",
        "\n",
        "    #make a list with alternating between a Denseblock and a TransitionLayer (one less than the wanten number of blocks as the last layer should not be a transitional one)\n",
        "    self.block_list = [block for block in range(num_blocks-1) for block in (DenseBlock(block_length,growth_rate), TransitionLayer(num_blocks * growth_rate))]\n",
        "    self.block_list.append(DenseBlock(block_length,growth_rate))\n",
        "\n",
        "    self.average = GlobalAveragePooling2D()\n",
        "    self.dense = Dense(units= 10, activation= softmax)\n",
        "\n",
        "  @tf.function\n",
        "  def call(self, input, training = False):\n",
        "    print(\"a\")\n",
        "    x = self.conv_1(input)\n",
        "    x = self.batch_1(x,training = training)\n",
        "    x = self.activation_1(x)\n",
        "    for layer in self.block_list:\n",
        "      x = layer(x,training = training)\n",
        "    x = self.average(x)\n",
        "    x = self.dense(x)\n",
        "    return x"
      ],
      "execution_count": 951,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9eERBJLHsWh"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iza-0g2TF4B1"
      },
      "source": [
        "@tf.function\n",
        "def forward_step(model, img, label ,loss_func, optimizer, training=True):\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = model(img, training)\n",
        "        loss = loss_func(label, prediction) + tf.reduce_sum(model.losses)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # calc accuracy of the batch\n",
        "    sample_test_accuracy = tf.argmax(label, axis=1) == tf.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = tf.reduce_mean(tf.cast(sample_test_accuracy, tf.float32))  \n",
        "\n",
        "    return tf.reduce_mean(loss), sample_test_accuracy\n",
        "\n",
        "#@tf.function\n",
        "def test(model, test_data, loss_function):\n",
        "    # test over complete test data\n",
        "\n",
        "    test_accuracy_aggregator = []\n",
        "    test_loss_aggregator = []\n",
        "\n",
        "    for (data, target) in test_data:\n",
        "        prediction = model(data, False)\n",
        "        ## Calc Loss and its partial mean\n",
        "        sample_test_loss = loss_function(target, prediction)\n",
        "        test_loss_aggregator.append(np.mean(sample_test_loss.numpy()))\n",
        "        \n",
        "        ## Calc Accuracy and its partial mean\n",
        "        sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "        test_accuracy_aggregator.append(sample_test_accuracy)\n",
        "\n",
        "    test_loss = np.mean(test_loss_aggregator)\n",
        "    test_accuracy = np.mean(test_accuracy_aggregator)\n",
        "\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "# thanks to Cornelius who shared this function which I adjusted a bit\n",
        "def estimateTime(epoch, time_passed, number_of_epochs):\n",
        "    \"\"\"\n",
        "    estimates the time it will take until the network is done with training.\n",
        "    \"\"\"\n",
        "    epoch += 1\n",
        "    time_per_epoch = time_passed / epoch\n",
        "    number_of_epochs_left = number_of_epochs - epoch\n",
        "    time_sec_remaining = number_of_epochs_left * time_per_epoch\n",
        "    time_remain = str(datetime.timedelta(seconds=time_sec_remaining))\n",
        "    return time_remain\n",
        "\n",
        "# def save_best_weights(accuracy, best_so_far):\n",
        "#     '''saves the best model if acc > preset threshold and if it is the best one so far'''\n",
        "#     global SAVING_THRESHOLD\n",
        "#     if (accuracy > SAVING_THRESHOLD) and best_so_far:\n",
        "#         model.save(SAVE_PATH + str(int(accuracy*10000)))\n",
        "#         SAVING_THRESHOLD = accuracy\n",
        "#         return True\n",
        "#     return False"
      ],
      "execution_count": 952,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFb_roYBG5bg"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV-4Fc04F4ER"
      },
      "source": [
        "training_ds = [None, None]\n",
        "training_ds[0] = build_pipeline(training_raw, BATCH_SIZE, SHUFFLE_BUFFER, horizontal_flip=True)\n",
        "training_ds[1] = build_pipeline(training_raw, BATCH_SIZE, SHUFFLE_BUFFER, horizontal_flip=True, sample_pairing=True)\n",
        "test_ds = build_pipeline(test_raw, BATCH_SIZE, SHUFFLE_BUFFER)\n",
        "validation_ds = build_pipeline(validation_raw, BATCH_SIZE, SHUFFLE_BUFFER)"
      ],
      "execution_count": 953,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2oG2jg-F4G3",
        "outputId": "8f8c06ec-9182-43bc-db43-fa311b9e741f"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "startTime = time.time()\n",
        "print(f\"Initialize, {datetime.timedelta(seconds=startTime)}\")\n",
        "\n",
        "# Initialize NN\n",
        "#shift between ResNet and DenseNet\n",
        "#model = ResNet(block_num= 5,block_channels=32,num_layer_iterations=4)\n",
        "model = DenseNet()\n",
        "\n",
        "# lists for vizualisation\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# let's test our Model how it performes on the test dataset before learning\n",
        "test_loss, test_accuracy = test(model, test_ds, LOSS_FUNCTION)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "# check how our Model performs on training dataset before learning\n",
        "training_loss, training_accuracy = test(model, training_ds[0], LOSS_FUNCTION)\n",
        "training_losses.append(training_loss)\n",
        "training_accuracies.append(training_accuracy)\n",
        "\n",
        "print(f\"Start Training for {EPOCHS} Epochs\", f\"Training Loss: {np.round(float(training_loss), 3)}, Training Accuracy: {np.round(training_accuracy*100, 3)}%, Test Loss: {np.round(float(test_loss),3)}, Test Accuracy: {np.round(test_accuracy*100, 3)}%\", sep=\"\\n\")\n",
        "\n",
        "startTime = time.time()\n",
        "# We train for before specified epochs.\n",
        "for epoch in range(EPOCHS):\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"-\"*20)\n",
        "    print('Epoch: __ ', (epoch + 1))#, \", Pairing = \" + str(bool(PHASECONTROLLER[epoch])))\n",
        "\n",
        "    # perform a training step with each entry of our data pipeline and record loss and accuracy\n",
        "    t_acc_accumulator = []\n",
        "    for (data, label) in training_ds[PHASECONTROLLER[epoch]]:\n",
        "        training_loss, training_accuracy = forward_step(model, data, label, LOSS_FUNCTION, OPTIMIZER)\n",
        "        t_acc_accumulator.append(training_accuracy)\n",
        "\n",
        "    # save loss and accuracy\n",
        "    training_losses.append(training_loss)\n",
        "    training_accuracies.append(np.mean(t_acc_accumulator))\n",
        "\n",
        "    # check how our Model performs after one learning epoch on our test dataset and record Loss and Acc. as well\n",
        "    test_loss, test_accuracy = test(model, test_ds, LOSS_FUNCTION)\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    # check if we have a new best score and save the model if it is good enough\n",
        "    # if save_best_weights(test_accuracy, np.max(test_accuracies) < test_accuracy):\n",
        "    #     print(f\"CONGRATS, NEW BEST ACCURACY OF {np.round(test_accuracy, 3)}. MODEL WAS SAVED!\")\n",
        "    \n",
        "    test_accuracies.append(test_accuracy)\n",
        "    print(f\"Training Loss: {np.round(float(training_loss), 3)}, Training Accuracy: {np.round(training_accuracies[epoch]*100, 3)}%, Test Loss: {np.round(float(test_loss), 3)}, Test Accuracy: {np.round(test_accuracy*100, 3)}%, Finish in: {str(estimateTime(epoch, time.time() - startTime, EPOCHS))[:7]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialize, 18605 days, 22:48:31.287508\n",
            "a\n",
            "a\n",
            "a\n",
            "Start Training for 115 Epochs\n",
            "Training Loss: 2.308, Training Accuracy: 10.146%, Test Loss: 2.308, Test Accuracy: 10.107%\n",
            "--------------------\n",
            "Epoch: __  1\n",
            "a\n",
            "a\n",
            "Training Loss: 8.301, Training Accuracy: 10.146%, Test Loss: 2.278, Test Accuracy: 13.054%, Finish in: 0:16:41\n",
            "Epoch: __  2\n",
            "Training Loss: 8.002, Training Accuracy: 26.167%, Test Loss: 2.3, Test Accuracy: 16.535%, Finish in: 0:11:36\n",
            "Epoch: __  3\n",
            "Training Loss: 7.697, Training Accuracy: 36.669%, Test Loss: 2.303, Test Accuracy: 18.038%, Finish in: 0:10:17\n",
            "Epoch: __  4\n",
            "Training Loss: 7.38, Training Accuracy: 40.724%, Test Loss: 2.172, Test Accuracy: 24.031%, Finish in: 0:09:36\n",
            "Epoch: __  5\n",
            "Training Loss: 7.702, Training Accuracy: 45.609%, Test Loss: 1.81, Test Accuracy: 34.949%, Finish in: 0:09:09\n",
            "--------------------\n",
            "Epoch: __  6\n",
            "Training Loss: 7.439, Training Accuracy: 47.943%, Test Loss: 1.518, Test Accuracy: 45.767%, Finish in: 0:08:50\n",
            "Epoch: __  7\n",
            "Training Loss: 7.179, Training Accuracy: 49.387%, Test Loss: 1.399, Test Accuracy: 50.257%, Finish in: 0:08:36\n",
            "Epoch: __  8\n",
            "Training Loss: 6.216, Training Accuracy: 50.574%, Test Loss: 1.294, Test Accuracy: 54.074%, Finish in: 0:08:23\n",
            "Epoch: __  9\n",
            "Training Loss: 7.259, Training Accuracy: 52.136%, Test Loss: 1.249, Test Accuracy: 55.795%, Finish in: 0:08:13\n",
            "Epoch: __  10\n",
            "Training Loss: 6.729, Training Accuracy: 54.114%, Test Loss: 1.203, Test Accuracy: 57.812%, Finish in: 0:08:03\n",
            "--------------------\n",
            "Epoch: __  11\n",
            "Training Loss: 6.041, Training Accuracy: 55.162%, Test Loss: 1.169, Test Accuracy: 58.861%, Finish in: 0:07:55\n",
            "Epoch: __  12\n",
            "Training Loss: 6.686, Training Accuracy: 56.922%, Test Loss: 1.12, Test Accuracy: 61.135%, Finish in: 0:07:48\n",
            "Epoch: __  13\n",
            "Training Loss: 5.8, Training Accuracy: 57.733%, Test Loss: 1.108, Test Accuracy: 60.779%, Finish in: 0:07:41\n",
            "Epoch: __  14\n",
            "Training Loss: 6.349, Training Accuracy: 59.256%, Test Loss: 1.092, Test Accuracy: 61.748%, Finish in: 0:07:34\n",
            "Epoch: __  15\n",
            "Training Loss: 5.836, Training Accuracy: 58.801%, Test Loss: 1.055, Test Accuracy: 63.133%, Finish in: 0:07:28\n",
            "--------------------\n",
            "Epoch: __  16\n",
            "Training Loss: 5.658, Training Accuracy: 60.483%, Test Loss: 1.039, Test Accuracy: 63.845%, Finish in: 0:07:22\n",
            "Epoch: __  17\n",
            "Training Loss: 6.264, Training Accuracy: 61.155%, Test Loss: 0.996, Test Accuracy: 65.19%, Finish in: 0:07:16\n",
            "Epoch: __  18\n",
            "Training Loss: 5.261, Training Accuracy: 62.54%, Test Loss: 0.984, Test Accuracy: 66.713%, Finish in: 0:07:10\n",
            "Epoch: __  19\n",
            "Training Loss: 6.208, Training Accuracy: 62.797%, Test Loss: 0.967, Test Accuracy: 65.902%, Finish in: 0:07:02\n",
            "Epoch: __  20\n",
            "Training Loss: 5.814, Training Accuracy: 64.201%, Test Loss: 0.944, Test Accuracy: 67.009%, Finish in: 0:06:53\n",
            "--------------------\n",
            "Epoch: __  21\n",
            "Training Loss: 5.52, Training Accuracy: 64.537%, Test Loss: 0.95, Test Accuracy: 67.168%, Finish in: 0:06:49\n",
            "Epoch: __  22\n",
            "Training Loss: 5.306, Training Accuracy: 65.704%, Test Loss: 0.886, Test Accuracy: 69.205%, Finish in: 0:06:44\n",
            "Epoch: __  23\n",
            "Training Loss: 5.922, Training Accuracy: 67.049%, Test Loss: 0.893, Test Accuracy: 69.581%, Finish in: 0:06:39\n",
            "Epoch: __  24\n",
            "Training Loss: 4.998, Training Accuracy: 66.574%, Test Loss: 0.867, Test Accuracy: 69.976%, Finish in: 0:06:35\n",
            "Epoch: __  25\n",
            "Training Loss: 5.923, Training Accuracy: 68.434%, Test Loss: 0.839, Test Accuracy: 71.994%, Finish in: 0:06:30\n",
            "--------------------\n",
            "Epoch: __  26\n",
            "Training Loss: 5.44, Training Accuracy: 67.781%, Test Loss: 0.808, Test Accuracy: 72.508%, Finish in: 0:06:26\n",
            "Epoch: __  27\n",
            "Training Loss: 5.099, Training Accuracy: 69.6%, Test Loss: 0.797, Test Accuracy: 72.429%, Finish in: 0:06:21\n",
            "Epoch: __  28\n",
            "Training Loss: 4.82, Training Accuracy: 70.293%, Test Loss: 0.769, Test Accuracy: 74.644%, Finish in: 0:06:17\n",
            "Epoch: __  29\n",
            "Training Loss: 6.068, Training Accuracy: 71.282%, Test Loss: 0.75, Test Accuracy: 74.96%, Finish in: 0:06:10\n",
            "Epoch: __  30\n",
            "Training Loss: 5.443, Training Accuracy: 71.222%, Test Loss: 0.752, Test Accuracy: 74.407%, Finish in: 0:06:04\n",
            "--------------------\n",
            "Epoch: __  31\n",
            "Training Loss: 5.638, Training Accuracy: 71.361%, Test Loss: 0.706, Test Accuracy: 76.78%, Finish in: 0:06:00\n",
            "Epoch: __  32\n",
            "Training Loss: 5.956, Training Accuracy: 72.389%, Test Loss: 0.701, Test Accuracy: 76.266%, Finish in: 0:05:55\n",
            "Epoch: __  33\n",
            "Training Loss: 4.568, Training Accuracy: 73.279%, Test Loss: 0.682, Test Accuracy: 77.097%, Finish in: 0:05:51\n",
            "Epoch: __  34\n",
            "Training Loss: 4.416, Training Accuracy: 73.438%, Test Loss: 0.673, Test Accuracy: 77.591%, Finish in: 0:05:47\n",
            "Epoch: __  35\n",
            "Training Loss: 5.233, Training Accuracy: 75.04%, Test Loss: 0.662, Test Accuracy: 78.362%, Finish in: 0:05:43\n",
            "--------------------\n",
            "Epoch: __  36\n",
            "Training Loss: 4.536, Training Accuracy: 74.11%, Test Loss: 0.645, Test Accuracy: 78.461%, Finish in: 0:05:39\n",
            "Epoch: __  37\n",
            "Training Loss: 4.65, Training Accuracy: 75.633%, Test Loss: 0.652, Test Accuracy: 78.224%, Finish in: 0:05:34\n",
            "Epoch: __  38\n",
            "Training Loss: 4.974, Training Accuracy: 75.02%, Test Loss: 0.618, Test Accuracy: 79.866%, Finish in: 0:05:30\n",
            "Epoch: __  39\n",
            "Training Loss: 4.263, Training Accuracy: 76.444%, Test Loss: 0.593, Test Accuracy: 80.815%, Finish in: 0:05:24\n",
            "Epoch: __  40\n",
            "Training Loss: 4.699, Training Accuracy: 77.611%, Test Loss: 0.598, Test Accuracy: 80.004%, Finish in: 0:05:19\n",
            "--------------------\n",
            "Epoch: __  41\n",
            "Training Loss: 5.603, Training Accuracy: 77.532%, Test Loss: 0.634, Test Accuracy: 78.402%, Finish in: 0:05:15\n",
            "Epoch: __  42\n",
            "Training Loss: 4.482, Training Accuracy: 78.659%, Test Loss: 0.572, Test Accuracy: 81.092%, Finish in: 0:05:10\n",
            "Epoch: __  43\n",
            "Training Loss: 4.426, Training Accuracy: 78.382%, Test Loss: 0.562, Test Accuracy: 81.942%, Finish in: 0:05:06\n",
            "Epoch: __  44\n",
            "Training Loss: 4.528, Training Accuracy: 78.481%, Test Loss: 0.559, Test Accuracy: 81.507%, Finish in: 0:05:02\n",
            "Epoch: __  45\n",
            "Training Loss: 4.308, Training Accuracy: 79.114%, Test Loss: 0.528, Test Accuracy: 82.892%, Finish in: 0:04:58\n",
            "--------------------\n",
            "Epoch: __  46\n",
            "Training Loss: 4.563, Training Accuracy: 79.707%, Test Loss: 0.522, Test Accuracy: 83.683%, Finish in: 0:04:54\n",
            "Epoch: __  47\n",
            "Training Loss: 4.117, Training Accuracy: 79.47%, Test Loss: 0.491, Test Accuracy: 85.225%, Finish in: 0:04:49\n",
            "Epoch: __  48\n",
            "Training Loss: 4.22, Training Accuracy: 80.538%, Test Loss: 0.504, Test Accuracy: 83.623%, Finish in: 0:04:45\n",
            "Epoch: __  49\n",
            "Training Loss: 5.248, Training Accuracy: 80.696%, Test Loss: 0.475, Test Accuracy: 85.7%, Finish in: 0:04:40\n",
            "Epoch: __  50\n",
            "Training Loss: 4.34, Training Accuracy: 80.063%, Test Loss: 0.466, Test Accuracy: 85.819%, Finish in: 0:04:35\n",
            "--------------------\n",
            "Epoch: __  51\n",
            "Training Loss: 3.879, Training Accuracy: 82.1%, Test Loss: 0.484, Test Accuracy: 84.138%, Finish in: 0:04:31\n",
            "Epoch: __  52\n",
            "Training Loss: 4.786, Training Accuracy: 82.259%, Test Loss: 0.461, Test Accuracy: 85.225%, Finish in: 0:04:27\n",
            "Epoch: __  53\n",
            "Training Loss: 3.814, Training Accuracy: 81.804%, Test Loss: 0.432, Test Accuracy: 87.282%, Finish in: 0:04:23\n",
            "Epoch: __  54\n",
            "Training Loss: 4.585, Training Accuracy: 83.01%, Test Loss: 0.439, Test Accuracy: 86.531%, Finish in: 0:04:18\n",
            "Epoch: __  55\n",
            "Training Loss: 4.461, Training Accuracy: 83.248%, Test Loss: 0.423, Test Accuracy: 86.392%, Finish in: 0:04:14\n",
            "--------------------\n",
            "Epoch: __  56\n",
            "Training Loss: 3.843, Training Accuracy: 83.762%, Test Loss: 0.412, Test Accuracy: 87.362%, Finish in: 0:04:10\n",
            "Epoch: __  57\n",
            "Training Loss: 4.699, Training Accuracy: 84.711%, Test Loss: 0.384, Test Accuracy: 88.35%, Finish in: 0:04:06\n",
            "Epoch: __  58\n",
            "Training Loss: 4.769, Training Accuracy: 84.909%, Test Loss: 0.373, Test Accuracy: 89.478%, Finish in: 0:04:02\n",
            "Epoch: __  59\n",
            "Training Loss: 4.229, Training Accuracy: 85.641%, Test Loss: 0.38, Test Accuracy: 88.43%, Finish in: 0:03:57\n",
            "Epoch: __  60\n",
            "Training Loss: 5.204, Training Accuracy: 84.949%, Test Loss: 0.352, Test Accuracy: 89.794%, Finish in: 0:03:52\n",
            "--------------------\n",
            "Epoch: __  61\n",
            "Training Loss: 4.297, Training Accuracy: 85.581%, Test Loss: 0.357, Test Accuracy: 89.181%, Finish in: 0:03:48\n",
            "Epoch: __  62\n",
            "Training Loss: 3.934, Training Accuracy: 86.017%, Test Loss: 0.374, Test Accuracy: 88.35%, Finish in: 0:03:44\n",
            "Epoch: __  63\n",
            "Training Loss: 4.007, Training Accuracy: 86.135%, Test Loss: 0.33, Test Accuracy: 90.665%, Finish in: 0:03:40\n",
            "Epoch: __  64\n",
            "Training Loss: 3.967, Training Accuracy: 86.254%, Test Loss: 0.346, Test Accuracy: 90.17%, Finish in: 0:03:36\n",
            "Epoch: __  65\n",
            "Training Loss: 3.374, Training Accuracy: 86.986%, Test Loss: 0.321, Test Accuracy: 91.416%, Finish in: 0:03:32\n",
            "--------------------\n",
            "Epoch: __  66\n",
            "Training Loss: 4.084, Training Accuracy: 87.46%, Test Loss: 0.332, Test Accuracy: 89.735%, Finish in: 0:03:27\n",
            "Epoch: __  67\n",
            "Training Loss: 3.655, Training Accuracy: 87.619%, Test Loss: 0.299, Test Accuracy: 91.911%, Finish in: 0:03:23\n",
            "Epoch: __  68\n",
            "Training Loss: 4.069, Training Accuracy: 87.362%, Test Loss: 0.328, Test Accuracy: 90.348%, Finish in: 0:03:19\n",
            "Epoch: __  69\n",
            "Training Loss: 3.55, Training Accuracy: 88.192%, Test Loss: 0.281, Test Accuracy: 92.682%, Finish in: 0:03:14\n",
            "Epoch: __  70\n",
            "Training Loss: 3.559, Training Accuracy: 89.043%, Test Loss: 0.295, Test Accuracy: 91.634%, Finish in: 0:03:10\n",
            "--------------------\n",
            "Epoch: __  71\n",
            "Training Loss: 4.499, Training Accuracy: 88.41%, Test Loss: 0.303, Test Accuracy: 91.238%, Finish in: 0:03:06\n",
            "Epoch: __  72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBXUG1KOMZvm"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKIMKLLEF4JZ"
      },
      "source": [
        "# taken from Tensorflow_Intro.ipynb\n",
        "\n",
        "# Visualize accuracy and loss for training and test data. \n",
        "# One plot training and test loss.\n",
        "# One plot training and test accuracy.\n",
        "plt.figure()\n",
        "line1, = plt.plot(training_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend((line1,line2),(\"training\",\"test\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "line1, = plt.plot(training_accuracies)\n",
        "line2, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend((line1,line2),(\"training\", \"test\"))\n",
        "plt.show()\n",
        "print(f\"Max Accuracy: {np.round(np.max(test_accuracies)*100, 3)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDI9oC7F4ME"
      },
      "source": [
        "# # load and compile our best model\n",
        "# model = tf.keras.models.load_model(SAVE_PATH+str(int(SAVING_THRESHOLD*10000)))\n",
        "# model.compile()\n",
        "\n",
        "# test their performance once again\n",
        "_, test_acc = test(model, validation_ds, LOSS_FUNCTION)\n",
        "_, train_acc = test(model, training_ds[0], LOSS_FUNCTION)\n",
        "\n",
        "labels = 'Correct', 'Wrong'\n",
        "explode = (0, 0.1)\n",
        "\n",
        "# cook some cakes\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie([train_acc, 1-train_acc], explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=45)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title(\"Accuracy on Train Dataset\", fontdict={\"fontsize\":20})\n",
        "plt.show()\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie([test_acc, 1-test_acc], explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=45)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title(\"Accuracy on Test Dataset\", fontdict={\"fontsize\":20})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}