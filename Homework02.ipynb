{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D0HUwWAEKeM"
   },
   "source": [
    "Homework02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7euT0HdQEKeR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the activation function: sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the derivative of the activation function\n",
    "def sigmoidprime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8bO7NpCDEKej"
   },
   "outputs": [],
   "source": [
    "# four possible input pairs of (x1,x2)\n",
    "possible_inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "# target of the logical gates\n",
    "t_and = np.array([0,0,0,1])\n",
    "t_or = np.array([0,1,1,1])\n",
    "t_nand = np.array([1,1,1,0])\n",
    "t_nor = np.array([1,0,0,0])\n",
    "t_xor = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_PT3Q_gEKer"
   },
   "source": [
    "#### Implement Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pDWFmomzEKet"
   },
   "outputs": [],
   "source": [
    "# implement the perceptron class\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_units):\n",
    "        self.input_units = input_units\n",
    "        \n",
    "        # Initialize random weights and a random bias term\n",
    "        self.weights = np.random.randn(input_units)\n",
    "        self.bias = np.random.randn()\n",
    "        # Define the learning rate as 0.01\n",
    "        self.alpha = 0.01\n",
    "        \n",
    "        # initialize the variable for later use\n",
    "        self.inputs = 0 \n",
    "        \n",
    "    def forward_step(self, inputs):\n",
    "        # Perform a perceptron forward step:\n",
    "                  \n",
    "        # 1. Calculate the drive (use @ as a matrix multiplication command)\n",
    "        weighted_sum =  self.weights @ inputs + self.bias \n",
    "        # An alternative would be:\n",
    "        # weighted_sum =  np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "        # 2. Return the activation\n",
    "        return sigmoid(weighted_sum)\n",
    "    \n",
    "    def update(self, delta):\n",
    "        \n",
    "        # calculate the weight updates\n",
    "        gradient_weights = delta * self.inputs\n",
    "        self.weights -= self.alpha * gradient_weights\n",
    "        # calculate the bias updates\n",
    "        gradient_bias = delta\n",
    "        self.bias -= self.alpha * gradient_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Multi-Layer Perceptron class which can perform a forward and backprop-step\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self):\n",
    "        # initialize 4 perceptrons of the hidden layer\n",
    "        self.h_layer = [Perceptron(2), Perceptron(2), Perceptron(2), Perceptron(2)]\n",
    "        \n",
    "        # initialize the output neuron\n",
    "        self.out_neuron = Perceptron(4)\n",
    "        \n",
    "        # initialize a variable self.output to store the output\n",
    "        self.output = 0\n",
    "    \n",
    "    def forward_step(self, inputs):\n",
    "        # create an empty np array\n",
    "        activations = np.array([])\n",
    "        # compute the activation for every perceptron in hidden layer iteratively \n",
    "        for perceptron in self.h_layer:\n",
    "            # store the values in the activations array\n",
    "            activations.append(perceptron.forward_step(inputs))\n",
    "        # reshape the resulting array to feed it to the output neuron\n",
    "        activations = np.reshape(activations, newshape = (-1))\n",
    "        # feed the activations of the hidden layer into the output layer\n",
    "        # store it in self.output\n",
    "        self.output = self.out_neuron.forward_step(activations)\n",
    "    \n",
    "    def backprop_step(self, inputs, target):\n",
    "        delta_output = (self.output - target) * sigmoidprime(self.out_neuron.weighted_sum)\n",
    "        self.out_neuron.update(delta_output)\n",
    "        for index, perceptron in enumerate(self.h_layer):\n",
    "             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Flipped_classroom02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
